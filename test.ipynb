{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 17:54:59.292124: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-29 17:54:59.898100: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-29 17:55:00.253861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743270900.736874   29255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743270900.867428   29255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743270901.587034   29255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743270901.587067   29255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743270901.587070   29255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743270901.587073   29255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-29 17:55:01.655979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-03-29 17:55:16.836753: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "# Normalize the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Plot the first test image and its prediction\n",
    "plt.imshow(x_test[0], cmap=plt.cm.binary)\n",
    "plt.title(f\"Predicted: {tf.argmax(predictions[0]).numpy()}, Actual: {y_test[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sample data for chatbot (input-output pairs)\n",
    "input_texts = [\"hello\", \"how are you\", \"what is your name\", \"bye\"]\n",
    "target_texts = [\"hi\", \"I am fine, thank you\", \"I am a chatbot\", \"goodbye\"]\n",
    "\n",
    "# Tokenize the data\n",
    "input_characters = sorted(set(\"\".join(input_texts)))\n",
    "target_characters = sorted(set(\"\".join(target_texts)))\n",
    "\n",
    "input_token_index = {char: i for i, char in enumerate(input_characters)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_characters)}\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, len(input_characters)), dtype=\"float32\")\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, len(target_characters)), dtype=\"float32\")\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, len(target_characters)), dtype=\"float32\")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "\n",
    "# Build the model\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, len(input_characters)))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, len(target_characters)))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(target_characters), activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=100)\n",
    "\n",
    "# Inference models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Decode function\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, len(target_characters)))\n",
    "    target_seq[0, 0, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = list(target_token_index.keys())[list(target_token_index.values()).index(sampled_token_index)]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, len(target_characters)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Test the chatbot\n",
    "test_input = \"hello\"\n",
    "test_input_seq = np.zeros((1, max_encoder_seq_length, len(input_characters)), dtype=\"float32\")\n",
    "for t, char in enumerate(test_input):\n",
    "    test_input_seq[0, t, input_token_index[char]] = 1.0\n",
    "\n",
    "decoded_sentence = decode_sequence(test_input_seq)\n",
    "print(f\"Chatbot response: {decoded_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the dataset\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, input_token_index, target_token_index, max_input_len, max_target_len):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.input_token_index = input_token_index\n",
    "        self.target_token_index = target_token_index\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = torch.zeros(self.max_input_len, len(self.input_token_index))\n",
    "        target_seq = torch.zeros(self.max_target_len, len(self.target_token_index))\n",
    "\n",
    "        for t, char in enumerate(self.input_texts[idx]):\n",
    "            input_seq[t, self.input_token_index[char]] = 1.0\n",
    "        for t, char in enumerate(self.target_texts[idx]):\n",
    "            target_seq[t, self.target_token_index[char]] = 1.0\n",
    "\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        output = self.fc(output)\n",
    "        return output, hidden, cell\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
    "        batch_size, target_len, _ = target_seq.size()\n",
    "        outputs = torch.zeros_like(target_seq)\n",
    "\n",
    "        hidden, cell = self.encoder(input_seq)\n",
    "        decoder_input = target_seq[:, 0, :].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            decoder_input = target_seq[:, t, :].unsqueeze(1) if teacher_force else output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(input_token_index)\n",
    "output_size = len(target_token_index)\n",
    "hidden_size = 256\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "dataset = ChatDataset(input_texts, target_texts, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, target_seq)\n",
    "        loss = criterion(output.view(-1, output_size), target_seq.view(-1, output_size))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test the chatbot\n",
    "def predict(input_text):\n",
    "    input_seq = torch.zeros(1, max_encoder_seq_length, len(input_token_index))\n",
    "    for t, char in enumerate(input_text):\n",
    "        input_seq[0, t, input_token_index[char]] = 1.0\n",
    "\n",
    "    hidden, cell = encoder(input_seq)\n",
    "    decoder_input = torch.zeros(1, 1, len(target_token_index))\n",
    "    decoder_input[0, 0, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "    decoded_sentence = \"\"\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        sampled_token_index = output.argmax(2).item()\n",
    "        sampled_char = list(target_token_index.keys())[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        if sampled_char == \"\\n\":\n",
    "            break\n",
    "        decoder_input = torch.zeros(1, 1, len(target_token_index))\n",
    "        decoder_input[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "test_input = \"hello\"\n",
    "response = predict(test_input)\n",
    "print(f\"Chatbot response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, Response, request, send_file\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Path to the directory containing video files\n",
    "VIDEO_DIR = \"videos\"\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    # List all video files in the directory\n",
    "    videos = os.listdir(VIDEO_DIR)\n",
    "    return render_template('index.html', videos=videos)\n",
    "\n",
    "@app.route('/video/<filename>')\n",
    "def video(filename):\n",
    "    # Serve the video file\n",
    "    return send_file(os.path.join(VIDEO_DIR, filename))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure the video directory exists\n",
    "    if not os.path.exists(VIDEO_DIR):\n",
    "        os.makedirs(VIDEO_DIR)\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    \"Google\": \"Google is a search engine.\",\n",
    "    \"Python\": \"Python is a programming language.\",\n",
    "    \"Flask\": \"Flask is a web framework for Python.\",\n",
    "    \"AI\": \"AI stands for Artificial Intelligence.\"\n",
    "}\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('search.html')\n",
    "\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('q', '').lower()\n",
    "    results = {key: value for key, value in data.items() if query in key.lower() or query in value.lower()}\n",
    "    return render_template('results.html', query=query, results=results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
